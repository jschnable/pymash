{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From VCF to Pleiotropic Markers: A Complete GWAS + mash Pipeline\n",
    "\n",
    "This notebook walks through a complete analysis starting from raw inputs\n",
    "(a VCF genotype file and a phenotype file) through per-trait GWAS using\n",
    "[PANICLE](https://pypi.org/project/panicle/) and then multi-trait shrinkage\n",
    "with pymash to identify markers with strong **pleiotropic** effects.\n",
    "\n",
    "**Additional dependencies:** This notebook requires packages not included\n",
    "in pymash's core dependencies. Install them with:\n",
    "\n",
    "```bash\n",
    "pip install panicle pandas matplotlib\n",
    "```\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You have:\n",
    "- A VCF file with genotypes for hundreds of individuals at millions of markers\n",
    "- A phenotype file with measurements for multiple traits\n",
    "\n",
    "You want to find markers that affect **multiple traits simultaneously** (pleiotropy),\n",
    "with properly calibrated significance measures.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Load data** — VCF + phenotypes via PANICLE\n",
    "2. **Run per-trait GWAS** — get effect sizes and standard errors per marker per trait\n",
    "3. **Build Bhat/Shat matrices** — assemble GWAS results into the format mash expects\n",
    "4. **Run mash** — joint multi-trait shrinkage\n",
    "5. **Identify pleiotropic markers** — find markers significant across multiple traits\n",
    "\n",
    "---\n",
    "\n",
    "Since we don't have a real 10M-marker VCF in this demo, we'll simulate\n",
    "genotype and phenotype data that mimics the structure of a real dataset.\n",
    "All the PANICLE and pymash calls are exactly what you'd use on real data —\n",
    "only the data generation step would be replaced by file loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:33.677004Z",
     "iopub.status.busy": "2026-02-10T12:20:33.676856Z",
     "iopub.status.idle": "2026-02-10T12:20:34.908603Z",
     "shell.execute_reply": "2026-02-10T12:20:34.908080Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import panicle\n",
    "from panicle.utils.data_types import GenotypeMatrix\n",
    "from panicle.data.loaders import load_genotype_vcf, load_phenotype_file, match_individuals\n",
    "\n",
    "import pymash as mash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data with PANICLE\n",
    "\n",
    "### Loading from real files\n",
    "\n",
    "With real data, you would load your VCF and phenotype file like this:\n",
    "\n",
    "```python\n",
    "# Load genotypes from VCF (handles .vcf and .vcf.gz)\n",
    "geno, sample_ids, geno_map = load_genotype_vcf(\n",
    "    \"my_genotypes.vcf.gz\",\n",
    "    min_maf=0.05,          # filter rare variants\n",
    "    max_missing=0.2,       # drop markers with >20% missing\n",
    "    drop_monomorphic=True, # drop invariant sites\n",
    ")\n",
    "print(f\"Loaded {geno.n_markers} markers for {geno.n_individuals} individuals\")\n",
    "\n",
    "# Load phenotypes (CSV/TSV with ID column + trait columns)\n",
    "pheno_df = load_phenotype_file(\"phenotypes.csv\")\n",
    "trait_names = [c for c in pheno_df.columns if c != \"ID\"]\n",
    "print(f\"Traits: {trait_names}\")\n",
    "\n",
    "# Match individuals between genotype and phenotype\n",
    "pheno_matched, _, keep_idx, match_info = match_individuals(pheno_df, sample_ids)\n",
    "print(f\"Matched {match_info['n_common']} individuals\")\n",
    "```\n",
    "\n",
    "### For this demo: simulate data\n",
    "\n",
    "We'll simulate 500 individuals, 5000 markers, and 8 traits with known\n",
    "pleiotropic architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:34.909950Z",
     "iopub.status.busy": "2026-02-10T12:20:34.909826Z",
     "iopub.status.idle": "2026-02-10T12:20:34.954515Z",
     "shell.execute_reply": "2026-02-10T12:20:34.954138Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Simulate genotype and phenotype data ---\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "n_ind = 500      # individuals\n",
    "n_markers = 5000 # markers (use millions for real data)\n",
    "n_traits = 8     # phenotypes\n",
    "\n",
    "# Simulate genotype matrix (0/1/2 dosage) with varying allele frequencies\n",
    "maf = rng.uniform(0.05, 0.5, size=n_markers)\n",
    "geno_raw = np.zeros((n_ind, n_markers), dtype=np.int8)\n",
    "for j in range(n_markers):\n",
    "    geno_raw[:, j] = rng.binomial(2, maf[j], size=n_ind)\n",
    "\n",
    "# Simulate true effects: most markers are null, some are pleiotropic\n",
    "n_causal = 50  # causal markers\n",
    "causal_idx = rng.choice(n_markers, size=n_causal, replace=False)\n",
    "\n",
    "true_effects = np.zeros((n_markers, n_traits))\n",
    "for i, idx in enumerate(causal_idx):\n",
    "    if i < 15:\n",
    "        # Group 1: affect all traits (fully pleiotropic)\n",
    "        true_effects[idx, :] = rng.normal(0, 0.3, size=n_traits)\n",
    "    elif i < 30:\n",
    "        # Group 2: affect traits 0-3 only (block pleiotropic)\n",
    "        true_effects[idx, :4] = rng.normal(0, 0.3, size=4)\n",
    "    elif i < 40:\n",
    "        # Group 3: affect trait 0 only (trait-specific)\n",
    "        true_effects[idx, 0] = rng.normal(0, 0.5)\n",
    "    else:\n",
    "        # Group 4: affect traits 4-7 only\n",
    "        true_effects[idx, 4:] = rng.normal(0, 0.3, size=4)\n",
    "\n",
    "# Simulate phenotypes: Y = X @ beta + noise\n",
    "X_std = (geno_raw - geno_raw.mean(axis=0)) / np.maximum(geno_raw.std(axis=0), 1e-8)\n",
    "Y = X_std @ true_effects + rng.normal(0, 1.0, size=(n_ind, n_traits))\n",
    "\n",
    "print(f\"Genotypes: {n_ind} individuals x {n_markers} markers\")\n",
    "print(f\"Phenotypes: {n_ind} individuals x {n_traits} traits\")\n",
    "print(f\"Causal markers: {n_causal} (of which 15 fully pleiotropic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap data in PANICLE types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:34.969202Z",
     "iopub.status.busy": "2026-02-10T12:20:34.969109Z",
     "iopub.status.idle": "2026-02-10T12:20:34.986070Z",
     "shell.execute_reply": "2026-02-10T12:20:34.985672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create PANICLE-compatible objects\n",
    "sample_ids = [f\"IND_{i:04d}\" for i in range(n_ind)]\n",
    "marker_ids = [f\"MARKER_{j:06d}\" for j in range(n_markers)]\n",
    "trait_names = [f\"Trait_{t}\" for t in range(n_traits)]\n",
    "\n",
    "# GenotypeMatrix\n",
    "geno = GenotypeMatrix(geno_raw)\n",
    "\n",
    "# Marker map with columns: MARKER, CHROM, POS\n",
    "map_df = pd.DataFrame({\n",
    "    \"MARKER\": marker_ids,\n",
    "    \"CHROM\": np.repeat(np.arange(1, 11), n_markers // 10),  # 10 chromosomes\n",
    "    \"POS\": np.tile(np.arange(n_markers // 10) * 1000, 10),\n",
    "})\n",
    "geno_map = map_df\n",
    "\n",
    "print(f\"GenotypeMatrix: {geno.n_individuals} x {geno.n_markers}\")\n",
    "print(f\"Marker map: {len(geno_map)} markers on {geno_map['CHROM'].nunique()} chromosomes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Run Per-Trait GWAS with PANICLE\n",
    "\n",
    "We run GWAS independently for each trait using PANICLE's Mixed Linear Model\n",
    "(MLM), which controls for population structure via a kinship matrix.\n",
    "\n",
    "For each trait, the output includes:\n",
    "- **Effect sizes** (betas) — goes into mash's `Bhat`\n",
    "- **Standard errors** — goes into mash's `Shat`\n",
    "\n",
    "### Compute kinship matrix once\n",
    "\n",
    "The kinship matrix captures genetic relatedness and is shared across all traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:34.987137Z",
     "iopub.status.busy": "2026-02-10T12:20:34.987067Z",
     "iopub.status.idle": "2026-02-10T12:20:34.994988Z",
     "shell.execute_reply": "2026-02-10T12:20:34.994593Z"
    }
   },
   "outputs": [],
   "source": [
    "K = panicle.PANICLE_K_VanRaden(geno, verbose=False)\n",
    "print(f\"Kinship matrix: {K.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLM for each trait\n",
    "\n",
    "We loop over traits and collect effect sizes and standard errors into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:34.996031Z",
     "iopub.status.busy": "2026-02-10T12:20:34.995967Z",
     "iopub.status.idle": "2026-02-10T12:20:35.315914Z",
     "shell.execute_reply": "2026-02-10T12:20:35.315578Z"
    }
   },
   "outputs": [],
   "source": [
    "Bhat = np.zeros((n_markers, n_traits))\n",
    "Shat = np.zeros((n_markers, n_traits))\n",
    "\n",
    "for t in range(n_traits):\n",
    "    # Format phenotype as PANICLE expects: (n x 2) array of [ID, value]\n",
    "    phe_array = np.column_stack([\n",
    "        np.arange(n_ind),  # individual indices\n",
    "        Y[:, t]            # trait values\n",
    "    ])\n",
    "    \n",
    "    # Run MLM\n",
    "    result = panicle.PANICLE_MLM(\n",
    "        phe=phe_array,\n",
    "        geno=geno,\n",
    "        K=K,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    Bhat[:, t] = result.effects\n",
    "    Shat[:, t] = result.se\n",
    "    \n",
    "    n_sig = np.sum(result.pvalues < 5e-8)\n",
    "    print(f\"  {trait_names[t]}: {n_sig} genome-wide significant markers\")\n",
    "\n",
    "print(f\"\\nBhat shape: {Bhat.shape}\")\n",
    "print(f\"Shat shape: {Shat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up GWAS output and convert to z-scores\n",
    "\n",
    "Before passing to mash, handle any edge cases in the GWAS output, then\n",
    "convert to **z-scores** (Bhat / Shat) with unit standard errors. Working in\n",
    "z-score space is standard practice (equivalent to `alpha=1` in R mashr)\n",
    "and enables efficient computation in pymash. Posterior means can be\n",
    "converted back to effect-size scale afterwards by multiplying by the\n",
    "original Shat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:35.317029Z",
     "iopub.status.busy": "2026-02-10T12:20:35.316968Z",
     "iopub.status.idle": "2026-02-10T12:20:35.319712Z",
     "shell.execute_reply": "2026-02-10T12:20:35.319415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace any zero/near-zero SEs (can happen with monomorphic markers)\n",
    "min_se = 1e-6\n",
    "small_se = Shat < min_se\n",
    "if np.any(small_se):\n",
    "    print(f\"Replacing {np.sum(small_se)} near-zero SEs\")\n",
    "    Shat[small_se] = min_se\n",
    "    Bhat[small_se] = 0.0\n",
    "\n",
    "# Replace any NaN/Inf\n",
    "bad = ~np.isfinite(Bhat) | ~np.isfinite(Shat)\n",
    "if np.any(bad):\n",
    "    print(f\"Replacing {np.sum(bad)} non-finite values\")\n",
    "    Bhat[bad] = 0.0\n",
    "    Shat[bad] = 1e6  # large SE effectively makes this a missing value\n",
    "\n",
    "print(f\"Bhat range: [{Bhat.min():.4f}, {Bhat.max():.4f}]\")\n",
    "print(f\"Shat range: [{Shat.min():.4f}, {Shat.max():.4f}]\")\n",
    "\n",
    "# Convert to z-scores for mash analysis.\n",
    "# Posterior means can be converted back: pm_effect = pm_z * Shat\n",
    "Zhat = Bhat / Shat\n",
    "Shat_ones = np.ones_like(Shat)\n",
    "\n",
    "print(f\"\\nZ-score range: [{Zhat.min():.2f}, {Zhat.max():.2f}]\")\n",
    "print(f\"Max |Z|: {np.abs(Zhat).max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Run mash for Multi-Trait Shrinkage\n",
    "\n",
    "Now we have J markers x R traits matrices of z-scores (with unit standard errors).\n",
    "mash will:\n",
    "1. Learn which patterns of cross-trait sharing are present\n",
    "2. Shrink the estimates toward the appropriate pattern\n",
    "3. Provide calibrated significance measures (lfsr) across all traits jointly\n",
    "\n",
    "At the end, we convert posterior means back to the original effect-size scale.\n",
    "\n",
    "pymash provides a **workflow module** (`select_training_effects`,\n",
    "`fit_mash_prior`, `apply_mash_prior`, `mash_train_apply`) that handles\n",
    "the standard two-stage fitting pattern: train mixture weights on a\n",
    "subset, then apply the learned model to the full dataset.\n",
    "\n",
    "### Create strong and random subsets\n",
    "\n",
    "With millions of markers, you'd use subsets. With our 5000-marker demo\n",
    "we can use all markers, but we'll demonstrate the subsetting workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:35.320797Z",
     "iopub.status.busy": "2026-02-10T12:20:35.320744Z",
     "iopub.status.idle": "2026-02-10T12:20:36.993177Z",
     "shell.execute_reply": "2026-02-10T12:20:36.992751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify strong signals via condition-by-condition analysis\n",
    "full_data = mash.mash_set_data(Zhat, Shat_ones)\n",
    "m1 = mash.mash_1by1(full_data)\n",
    "strong_idx = mash.get_significant_results(m1, thresh=0.05)\n",
    "print(f\"Strong signals (lfsr < 0.05 in any trait): {len(strong_idx)}\")\n",
    "\n",
    "# Select a random training subset using the workflow module.\n",
    "# For real data with millions of markers, use n_train=20000-50000.\n",
    "random_idx = mash.select_training_effects(\n",
    "    full_data, n_train=min(n_markers, 5000), method=\"random\", seed=42,\n",
    ")\n",
    "print(f\"Random training subset size: {len(random_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate null correlations among traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:36.994299Z",
     "iopub.status.busy": "2026-02-10T12:20:36.994231Z",
     "iopub.status.idle": "2026-02-10T12:20:37.068605Z",
     "shell.execute_reply": "2026-02-10T12:20:37.068146Z"
    }
   },
   "outputs": [],
   "source": [
    "data_rand_temp = mash.mash_set_data(Zhat[random_idx], Shat_ones[random_idx])\n",
    "Vhat = mash.estimate_null_correlation_simple(data_rand_temp, z_thresh=2.0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(Vhat, cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(n_traits))\n",
    "ax.set_yticks(range(n_traits))\n",
    "ax.set_xticklabels(trait_names, rotation=45, ha=\"right\", fontsize=8)\n",
    "ax.set_yticklabels(trait_names, fontsize=8)\n",
    "ax.set_title(\"Estimated null correlation among traits\")\n",
    "fig.colorbar(im, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data objects with estimated correlations\n",
    "\n",
    "We need two `MashData` objects:\n",
    "- `data_all` — the full dataset (for model fitting and posterior computation)\n",
    "- `data_strong` — just the strong signals (for covariance learning only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:37.069575Z",
     "iopub.status.busy": "2026-02-10T12:20:37.069506Z",
     "iopub.status.idle": "2026-02-10T12:20:37.071777Z",
     "shell.execute_reply": "2026-02-10T12:20:37.071403Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all = mash.mash_set_data(Zhat, Shat_ones, V=Vhat)\n",
    "data_strong = mash.mash_set_data(Zhat[strong_idx], Shat_ones[strong_idx], V=Vhat)\n",
    "\n",
    "print(f\"Full data: {data_all.n_effects} markers x {data_all.n_conditions} traits\")\n",
    "print(f\"Strong data: {data_strong.n_effects} markers x {data_strong.n_conditions} traits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn data-driven covariances from strong signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:37.072707Z",
     "iopub.status.busy": "2026-02-10T12:20:37.072651Z",
     "iopub.status.idle": "2026-02-10T12:20:37.087396Z",
     "shell.execute_reply": "2026-02-10T12:20:37.087031Z"
    }
   },
   "outputs": [],
   "source": [
    "# PCA-based initial covariances\n",
    "npc = min(5, data_strong.n_conditions)\n",
    "U_pca = mash.cov_pca(data_strong, npc=npc)\n",
    "\n",
    "# Refine with Extreme Deconvolution\n",
    "U_ed = mash.cov_ed(data_strong, U_pca)\n",
    "\n",
    "# Canonical covariances\n",
    "U_c = mash.cov_canonical(data_all)\n",
    "\n",
    "# Combine all\n",
    "U_all = {**U_ed, **U_c}\n",
    "print(f\"Total covariance matrices: {len(U_all)}\")\n",
    "print(f\"  Data-driven (ED): {list(U_ed.keys())}\")\n",
    "print(f\"  Canonical: {list(U_c.keys())[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the mash model on the training subset\n",
    "\n",
    "**The Crucial Rule**: fit on the random (unbiased) subset, not the strong signals.\n",
    "\n",
    "`fit_mash_prior` handles subsetting internally — pass the full `data_all`\n",
    "and the `random_idx` from `select_training_effects`. It fits with\n",
    "`outputlevel=1` (mixture weights only, no posteriors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:20:37.088377Z",
     "iopub.status.busy": "2026-02-10T12:20:37.088324Z",
     "iopub.status.idle": "2026-02-10T12:22:25.684684Z",
     "shell.execute_reply": "2026-02-10T12:22:25.684283Z"
    }
   },
   "outputs": [],
   "source": [
    "fitted_g, train_idx, train_result = mash.fit_mash_prior(\n",
    "    data_all, U_all, train_indices=random_idx,\n",
    ")\n",
    "print(f\"Log-likelihood (training): {train_result.loglik:.2f}\")\n",
    "print(f\"Training subset: {len(train_idx)} markers\")\n",
    "\n",
    "# What sharing patterns did mash learn?\n",
    "pi_cov = mash.get_estimated_pi(train_result, dimension=\"cov\")\n",
    "labels = [\"null\"] + list(U_all.keys())\n",
    "print(\"\\nTop mixture components:\")\n",
    "for name, w in sorted(zip(labels, pi_cov), key=lambda x: -x[1])[:8]:\n",
    "    if w > 0.005:\n",
    "        print(f\"  {name}: {w:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply learned model to all markers\n",
    "\n",
    "`apply_mash_prior` applies the fitted prior (mixture weights + covariances)\n",
    "to compute shrunken posteriors for every marker, using `fixg=True` internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:22:25.685933Z",
     "iopub.status.busy": "2026-02-10T12:22:25.685853Z",
     "iopub.status.idle": "2026-02-10T12:22:25.782404Z",
     "shell.execute_reply": "2026-02-10T12:22:25.782022Z"
    }
   },
   "outputs": [],
   "source": [
    "m_all = mash.apply_mash_prior(data_all, fitted_g)\n",
    "\n",
    "pm_z = mash.get_pm(m_all)    # posterior means in z-score space (J x R)\n",
    "lfsr = mash.get_lfsr(m_all)  # local false sign rates (J x R)\n",
    "\n",
    "# Convert posterior means back to the original effect-size scale\n",
    "pm = pm_z * Shat\n",
    "\n",
    "print(f\"Posterior means (z-score) shape: {pm_z.shape}\")\n",
    "print(f\"Posterior means (effect size) shape: {pm.shape}\")\n",
    "print(f\"LFSR shape: {lfsr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot alternative: `mash_train_apply`\n",
    "\n",
    "The steps above (select training set → fit prior → apply) can be combined\n",
    "into a single call with `mash_train_apply`:\n",
    "\n",
    "```python\n",
    "result = mash.mash_train_apply(\n",
    "    data_all, U_all,\n",
    "    n_train=20000,              # auto-select random training subset\n",
    "    select_method=\"random\",     # or \"topz_random\" for top-|z| + random mix\n",
    ")\n",
    "pm_z  = mash.get_pm(result.apply_result)\n",
    "lfsr  = mash.get_lfsr(result.apply_result)\n",
    "print(f\"Trained on {len(result.train_indices)} markers\")\n",
    "```\n",
    "\n",
    "### For large datasets: batch processing\n",
    "\n",
    "With 10M markers, apply the fitted model in batches to manage memory:\n",
    "\n",
    "```python\n",
    "batch_size = 50000\n",
    "all_pm_z, all_lfsr = [], []\n",
    "\n",
    "for start in range(0, Zhat.shape[0], batch_size):\n",
    "    end = min(start + batch_size, Zhat.shape[0])\n",
    "    data_batch = mash.mash_set_data(Zhat[start:end], Shat_ones[start:end], V=Vhat)\n",
    "    m_batch = mash.apply_mash_prior(data_batch, fitted_g)\n",
    "    all_pm_z.append(mash.get_pm(m_batch))\n",
    "    all_lfsr.append(mash.get_lfsr(m_batch))\n",
    "    print(f\"  Processed markers {start:,}-{end:,}\")\n",
    "\n",
    "pm_z = np.vstack(all_pm_z)\n",
    "lfsr = np.vstack(all_lfsr)\n",
    "\n",
    "# Convert back to effect-size scale\n",
    "pm = pm_z * Shat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Identify Pleiotropic Markers\n",
    "\n",
    "### Count significant traits per marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:22:25.783614Z",
     "iopub.status.busy": "2026-02-10T12:22:25.783544Z",
     "iopub.status.idle": "2026-02-10T12:22:25.785840Z",
     "shell.execute_reply": "2026-02-10T12:22:25.785578Z"
    }
   },
   "outputs": [],
   "source": [
    "lfsr_thresh = 0.05\n",
    "n_sig_traits = np.sum(lfsr < lfsr_thresh, axis=1)\n",
    "\n",
    "print(\"Markers by number of significant traits:\")\n",
    "for k in range(n_traits + 1):\n",
    "    count = np.sum(n_sig_traits == k)\n",
    "    if count > 0:\n",
    "        print(f\"  {k} traits: {count} markers\")\n",
    "\n",
    "print(f\"\\nPleiotropic (>=2 traits): {np.sum(n_sig_traits >= 2)} markers\")\n",
    "print(f\"Highly pleiotropic (>=4 traits): {np.sum(n_sig_traits >= 4)} markers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine top pleiotropic markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:22:25.786968Z",
     "iopub.status.busy": "2026-02-10T12:22:25.786905Z",
     "iopub.status.idle": "2026-02-10T12:22:25.789769Z",
     "shell.execute_reply": "2026-02-10T12:22:25.789385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rank markers by number of significant traits, then by min lfsr\n",
    "min_lfsr = np.min(lfsr, axis=1)\n",
    "sort_key = -(n_sig_traits * 1000 - min_lfsr)  # primary: n_traits, secondary: lfsr\n",
    "top_idx = np.argsort(sort_key)\n",
    "\n",
    "print(f\"{'Marker':<15} {'N traits':>8} {'Min LFSR':>10}  Significant in\")\n",
    "print(\"-\" * 70)\n",
    "for rank, idx in enumerate(top_idx[:15]):\n",
    "    sig_traits = [trait_names[t] for t in range(n_traits) if lfsr[idx, t] < lfsr_thresh]\n",
    "    is_causal = \"*\" if idx in causal_idx else \" \"\n",
    "    print(f\"{marker_ids[idx]}{is_causal} {n_sig_traits[idx]:>7}  {min_lfsr[idx]:>10.4f}  {', '.join(sig_traits)}\")\n",
    "\n",
    "print(\"\\n* = truly causal in simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize posterior effects for a top pleiotropic marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:22:25.790746Z",
     "iopub.status.busy": "2026-02-10T12:22:25.790689Z",
     "iopub.status.idle": "2026-02-10T12:22:25.855614Z",
     "shell.execute_reply": "2026-02-10T12:22:25.855237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Forest plot for the most pleiotropic marker\n",
    "best = top_idx[0]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "y = np.arange(n_traits)\n",
    "\n",
    "# Mash posterior (shrunken, converted to effect-size scale)\n",
    "psd = mash.get_psd(m_all)\n",
    "pm_best = pm[best]\n",
    "psd_best = psd[best] * Shat[best]  # convert posterior SD to effect-size scale\n",
    "axes[0].errorbar(pm_best, y, xerr=1.96 * psd_best, fmt=\"o\", color=\"C0\", capsize=3)\n",
    "axes[0].axvline(0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "axes[0].set_yticks(y)\n",
    "axes[0].set_yticklabels(trait_names)\n",
    "axes[0].set_xlabel(\"Effect size\")\n",
    "axes[0].set_title(f\"{marker_ids[best]} — mash posterior\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Raw GWAS estimates (unshrunken)\n",
    "axes[1].errorbar(Bhat[best], y, xerr=1.96 * Shat[best], fmt=\"o\", color=\"black\", capsize=3)\n",
    "axes[1].axvline(0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "axes[1].set_yticks(y)\n",
    "axes[1].set_yticklabels(trait_names)\n",
    "axes[1].set_xlabel(\"Effect size\")\n",
    "axes[1].set_title(f\"{marker_ids[best]} — raw GWAS\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise trait sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:22:25.856832Z",
     "iopub.status.busy": "2026-02-10T12:22:25.856771Z",
     "iopub.status.idle": "2026-02-10T12:22:25.931431Z",
     "shell.execute_reply": "2026-02-10T12:22:25.931016Z"
    }
   },
   "outputs": [],
   "source": [
    "sharing = mash.get_pairwise_sharing(m_all, factor=0.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(sharing, vmin=0, vmax=1, cmap=\"YlOrRd\")\n",
    "ax.set_xticks(range(n_traits))\n",
    "ax.set_yticks(range(n_traits))\n",
    "ax.set_xticklabels(trait_names, rotation=45, ha=\"right\", fontsize=9)\n",
    "ax.set_yticklabels(trait_names, fontsize=9)\n",
    "for i in range(n_traits):\n",
    "    for j in range(n_traits):\n",
    "        val = sharing[i, j]\n",
    "        if np.isfinite(val):\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", fontsize=7)\n",
    "fig.colorbar(im, shrink=0.8, label=\"Sharing proportion\")\n",
    "ax.set_title(\"Pairwise sharing of significant effects\\n(same sign & within factor 0.5)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare mash shrinkage to raw GWAS\n",
    "\n",
    "mash borrows strength across traits, so its estimates are more accurate\n",
    "than per-trait GWAS alone. Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:22:25.932534Z",
     "iopub.status.busy": "2026-02-10T12:22:25.932464Z",
     "iopub.status.idle": "2026-02-10T12:22:25.934665Z",
     "shell.execute_reply": "2026-02-10T12:22:25.934308Z"
    }
   },
   "outputs": [],
   "source": [
    "# RMSE vs true effects for causal markers\n",
    "rmse_raw = np.sqrt(np.mean((Bhat[causal_idx] - true_effects[causal_idx]) ** 2))\n",
    "rmse_mash = np.sqrt(np.mean((pm[causal_idx] - true_effects[causal_idx]) ** 2))\n",
    "\n",
    "print(f\"RMSE at causal markers:\")\n",
    "print(f\"  Raw GWAS:  {rmse_raw:.4f}\")\n",
    "print(f\"  mash:      {rmse_mash:.4f}\")\n",
    "print(f\"  Improvement: {(1 - rmse_mash / rmse_raw) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:22:25.935520Z",
     "iopub.status.busy": "2026-02-10T12:22:25.935459Z",
     "iopub.status.idle": "2026-02-10T12:22:26.007280Z",
     "shell.execute_reply": "2026-02-10T12:22:26.006957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scatter: mash posterior vs true effect (for one trait)\n",
    "t = 0\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "lim = max(np.abs(true_effects[causal_idx, t]).max(),\n",
    "          np.abs(Bhat[causal_idx, t]).max()) * 1.2\n",
    "\n",
    "axes[0].scatter(true_effects[causal_idx, t], Bhat[causal_idx, t], alpha=0.7, s=30)\n",
    "axes[0].plot([-lim, lim], [-lim, lim], \"k--\", linewidth=0.5)\n",
    "axes[0].set_xlabel(\"True effect\")\n",
    "axes[0].set_ylabel(\"Raw GWAS estimate\")\n",
    "axes[0].set_title(f\"{trait_names[t]}: Raw GWAS\")\n",
    "axes[0].set_xlim(-lim, lim)\n",
    "axes[0].set_ylim(-lim, lim)\n",
    "\n",
    "axes[1].scatter(true_effects[causal_idx, t], pm[causal_idx, t], alpha=0.7, s=30, color=\"C1\")\n",
    "axes[1].plot([-lim, lim], [-lim, lim], \"k--\", linewidth=0.5)\n",
    "axes[1].set_xlabel(\"True effect\")\n",
    "axes[1].set_ylabel(\"mash posterior mean\")\n",
    "axes[1].set_title(f\"{trait_names[t]}: mash\")\n",
    "axes[1].set_xlim(-lim, lim)\n",
    "axes[1].set_ylim(-lim, lim)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Export Results\n",
    "\n",
    "Save the results for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:22:26.008479Z",
     "iopub.status.busy": "2026-02-10T12:22:26.008413Z",
     "iopub.status.idle": "2026-02-10T12:22:26.021419Z",
     "shell.execute_reply": "2026-02-10T12:22:26.021023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"MARKER\": marker_ids,\n",
    "    \"Chr\": map_df[\"CHROM\"].values,\n",
    "    \"Pos\": map_df[\"POS\"].values,\n",
    "    \"n_sig_traits\": n_sig_traits,\n",
    "    \"min_lfsr\": min_lfsr,\n",
    "})\n",
    "\n",
    "# Add per-trait posterior means and lfsr\n",
    "for t, name in enumerate(trait_names):\n",
    "    results_df[f\"pm_{name}\"] = pm[:, t]\n",
    "    results_df[f\"lfsr_{name}\"] = lfsr[:, t]\n",
    "\n",
    "# Sort by pleiotropy\n",
    "results_df = results_df.sort_values(\n",
    "    [\"n_sig_traits\", \"min_lfsr\"], ascending=[False, True]\n",
    ")\n",
    "\n",
    "print(\"Top 10 pleiotropic markers:\")\n",
    "print(results_df.head(10)[[\"MARKER\", \"Chr\", \"Pos\", \"n_sig_traits\", \"min_lfsr\"]].to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "# results_df.to_csv(\"mash_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This pipeline:\n",
    "\n",
    "1. **Loaded genotypes** from a VCF file and phenotypes from a CSV using PANICLE\n",
    "2. **Ran per-trait MLM GWAS** with PANICLE, extracting Bhat and Shat matrices\n",
    "3. **Estimated null correlations** among traits from a random marker subset\n",
    "4. **Learned effect-sharing patterns** from strong signals (PCA + ED)\n",
    "5. **Fit the mash model** on a random training subset using `fit_mash_prior`\n",
    "6. **Applied the learned model** to all markers using `apply_mash_prior`\n",
    "7. **Identified pleiotropic markers** significant across multiple traits\n",
    "\n",
    "### Workflow module functions used\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `select_training_effects` | Select a random (or top-z + random) training subset |\n",
    "| `fit_mash_prior` | Fit mixture weights on the training subset (`outputlevel=1`) |\n",
    "| `apply_mash_prior` | Apply fitted model to all markers (`fixg=True`) |\n",
    "| `mash_train_apply` | One-shot wrapper combining all three steps |\n",
    "\n",
    "### Key advantages over per-trait GWAS alone\n",
    "\n",
    "- **Better calibrated**: mash accounts for the correlation structure among traits\n",
    "- **More powerful**: borrowing information across traits recovers effects too weak\n",
    "  to detect in any single trait\n",
    "- **Interpretable**: the mixture proportions reveal which sharing patterns dominate,\n",
    "  and the pairwise sharing matrix shows which traits tend to share effects\n",
    "\n",
    "### Scaling to 10M markers\n",
    "\n",
    "For a real maize dataset with 10M markers:\n",
    "\n",
    "- **GWAS step**: run PANICLE MLM per trait (parallelizable across traits/chromosomes)\n",
    "- **Strong set**: use ~5-30k LD-pruned lead markers (one per locus) for covariance learning\n",
    "- **Random set**: use `select_training_effects(data, n_train=20000)` for model fitting\n",
    "- **Posteriors**: apply `apply_mash_prior` in batches of 50k with the fitted model\n",
    "- **Memory**: each batch needs only the Bhat/Shat slice + the fitted model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
