{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling to Real eQTL Data\n",
    "\n",
    "In real-world applications (e.g., eQTL studies with millions of tests across\n",
    "dozens of tissues), it is impractical to load and fit all tests at once.\n",
    "This notebook demonstrates the recommended workflow using two subsets:\n",
    "\n",
    "- **Strong signals** — top eQTLs per gene, used for learning covariance patterns\n",
    "- **Random subset** — an unbiased sample of all tests, used for fitting the model\n",
    "\n",
    "In [Urbut et al. 2019](https://doi.org/10.1038/s41588-018-0268-8), the strong\n",
    "set contained ~16k tests and the random set ~20k tests.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "1. Estimate null correlations from the random subset\n",
    "2. Learn data-driven covariances from the strong subset\n",
    "3. Fit the model (mixture proportions) on the random subset\n",
    "4. Compute posteriors for any subset using the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.297349Z",
     "iopub.status.busy": "2026-02-10T06:15:27.297226Z",
     "iopub.status.idle": "2026-02-10T06:15:27.625025Z",
     "shell.execute_reply": "2026-02-10T06:15:27.624470Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymash as mash\n",
    "from pymash.correlation import estimate_null_correlation_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Data\n",
    "\n",
    "We simulate a larger dataset (40k tests) to mimic a real scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.626404Z",
     "iopub.status.busy": "2026-02-10T06:15:27.626286Z",
     "iopub.status.idle": "2026-02-10T06:15:27.630359Z",
     "shell.execute_reply": "2026-02-10T06:15:27.630013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tests: 40000\n"
     ]
    }
   ],
   "source": [
    "sim = mash.simple_sims(nsamp=10000, ncond=5, err_sd=1.0, seed=1)\n",
    "print(f\"Total tests: {sim['Bhat'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Strong and Random Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.645102Z",
     "iopub.status.busy": "2026-02-10T06:15:27.645001Z",
     "iopub.status.idle": "2026-02-10T06:15:27.656526Z",
     "shell.execute_reply": "2026-02-10T06:15:27.656147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong signals: 23176\n",
      "Random subset: 5000\n"
     ]
    }
   ],
   "source": [
    "# Find strong signals using a quick 1-by-1 analysis\n",
    "full_data = mash.mash_set_data(sim[\"Bhat\"], sim[\"Shat\"])\n",
    "m1 = mash.mash_1by1(full_data)\n",
    "strong_idx = mash.get_significant_results(m1, thresh=0.05)\n",
    "print(f\"Strong signals: {len(strong_idx)}\")\n",
    "\n",
    "# Select a random subset of 5000 tests\n",
    "rng = np.random.default_rng(42)\n",
    "random_idx = rng.choice(sim[\"Bhat\"].shape[0], size=5000, replace=False)\n",
    "print(f\"Random subset: {len(random_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Estimate Null Correlations\n",
    "\n",
    "Estimate the residual correlation structure among conditions from the random\n",
    "subset. This captures correlations due to confounders, not true effects.\n",
    "\n",
    "**Use the random subset** (not the strong subset, which may lack null tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.657610Z",
     "iopub.status.busy": "2026-02-10T06:15:27.657542Z",
     "iopub.status.idle": "2026-02-10T06:15:27.660015Z",
     "shell.execute_reply": "2026-02-10T06:15:27.659734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated null correlation matrix:\n",
      "[[ 1.000e+00  8.399e-02  4.904e-02 -6.857e-04  3.717e-02]\n",
      " [ 8.399e-02  1.000e+00  6.944e-02  5.545e-02  5.890e-02]\n",
      " [ 4.904e-02  6.944e-02  1.000e+00  1.988e-02  4.224e-02]\n",
      " [-6.857e-04  5.545e-02  1.988e-02  1.000e+00  5.643e-02]\n",
      " [ 3.717e-02  5.890e-02  4.224e-02  5.643e-02  1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "data_temp = mash.mash_set_data(\n",
    "    sim[\"Bhat\"][random_idx], sim[\"Shat\"][random_idx]\n",
    ")\n",
    "Vhat = estimate_null_correlation_simple(data_temp)\n",
    "print(\"Estimated null correlation matrix:\")\n",
    "print(np.array2string(Vhat, precision=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Objects with Estimated Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.661066Z",
     "iopub.status.busy": "2026-02-10T06:15:27.660996Z",
     "iopub.status.idle": "2026-02-10T06:15:27.663938Z",
     "shell.execute_reply": "2026-02-10T06:15:27.663589Z"
    }
   },
   "outputs": [],
   "source": [
    "data_random = mash.mash_set_data(\n",
    "    sim[\"Bhat\"][random_idx], sim[\"Shat\"][random_idx], V=Vhat\n",
    ")\n",
    "data_strong = mash.mash_set_data(\n",
    "    sim[\"Bhat\"][strong_idx], sim[\"Shat\"][strong_idx], V=Vhat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Learn Data-Driven Covariances from Strong Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.665014Z",
     "iopub.status.busy": "2026-02-10T06:15:27.664951Z",
     "iopub.status.idle": "2026-02-10T06:15:36.767155Z",
     "shell.execute_reply": "2026-02-10T06:15:36.766684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-driven covariances: ['ED_PCA_1', 'ED_PCA_2', 'ED_PCA_3', 'ED_PCA_4', 'ED_PCA_5', 'ED_tPCA']\n"
     ]
    }
   ],
   "source": [
    "U_pca = mash.cov_pca(data_strong, npc=5)\n",
    "U_ed = mash.cov_ed(data_strong, U_pca)\n",
    "print(f\"Data-driven covariances: {list(U_ed.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fit the Model on the Random Subset\n",
    "\n",
    "Fit using both canonical and data-driven covariances on the **random** subset.\n",
    "Use `outputlevel=1` to skip computing posteriors (saves time — we only need\n",
    "the mixture proportions from this step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:36.768230Z",
     "iopub.status.busy": "2026-02-10T06:15:36.768153Z",
     "iopub.status.idle": "2026-02-10T06:17:02.756485Z",
     "shell.execute_reply": "2026-02-10T06:17:02.756002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood: -40027.60\n"
     ]
    }
   ],
   "source": [
    "U_c = mash.cov_canonical(data_random)\n",
    "U_all = {**U_ed, **U_c}\n",
    "\n",
    "m_fit = mash.mash(data_random, Ulist=U_all, outputlevel=1)\n",
    "print(f\"Log-likelihood: {m_fit.loglik:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute Posteriors on the Strong Subset\n",
    "\n",
    "Apply the learned model (mixture proportions) to the strong signals.\n",
    "Use `g=m_fit.fitted_g` and `fixg=True` to skip re-estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:17:02.757828Z",
     "iopub.status.busy": "2026-02-10T06:17:02.757733Z",
     "iopub.status.idle": "2026-02-10T06:17:03.146285Z",
     "shell.execute_reply": "2026-02-10T06:17:03.145803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant effects in strong set: 2241\n",
      "\n",
      "lfsr (first 5 effects):\n",
      "[[0.    0.    0.    0.    0.001]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.    0.554 0.596 0.561 0.306]\n",
      " [0.    0.653 0.607 0.701 0.591]\n",
      " [0.562 0.688 0.    0.771 0.67 ]]\n"
     ]
    }
   ],
   "source": [
    "m_strong = mash.mash(data_strong, g=m_fit.fitted_g, fixg=True)\n",
    "\n",
    "sig = mash.get_significant_results(m_strong, thresh=0.05)\n",
    "print(f\"Significant effects in strong set: {len(sig)}\")\n",
    "print(f\"\\nlfsr (first 5 effects):\")\n",
    "print(mash.get_lfsr(m_strong)[:5].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Real Data from CSV\n",
    "\n",
    "In practice, you would load your Bhat and Shat matrices from files.\n",
    "For example, starting from CSV files where rows are tests and columns\n",
    "are conditions:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "bhat_df = pd.read_csv(\"bhat.csv\", index_col=0)\n",
    "shat_df = pd.read_csv(\"shat.csv\", index_col=0)\n",
    "\n",
    "data = mash.mash_set_data(bhat_df.values, shat_df.values)\n",
    "```\n",
    "\n",
    "The key requirement is that `Bhat` and `Shat` are aligned matrices with\n",
    "the same shape: J tests (rows) by R conditions (columns)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
