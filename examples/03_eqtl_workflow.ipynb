{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Scaling to Real eQTL Data\n\nIn real-world applications (e.g., eQTL studies with millions of tests across\ndozens of tissues), it is impractical to load and fit all tests at once.\nThis notebook demonstrates the recommended workflow using two subsets:\n\n- **Strong signals** — top eQTLs per gene, used for learning covariance patterns\n- **Random subset** — an unbiased sample of all tests, used for fitting the model\n\nIn [Urbut et al. 2019](https://doi.org/10.1038/s41588-018-0268-8), the strong\nset contained ~16k tests and the random set ~20k tests.\n\npymash's **workflow module** (`select_training_effects`, `fit_mash_prior`,\n`apply_mash_prior`) handles the two-stage train/apply pattern, including\nsubsetting, fitting with `outputlevel=1`, and applying with `fixg=True`.\n\n## Strategy\n\n1. Estimate null correlations from the random subset\n2. Learn data-driven covariances from the strong subset\n3. Fit the model (mixture proportions) on the random subset via `fit_mash_prior`\n4. Compute posteriors for any subset using `apply_mash_prior`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.297349Z",
     "iopub.status.busy": "2026-02-10T06:15:27.297226Z",
     "iopub.status.idle": "2026-02-10T06:15:27.625025Z",
     "shell.execute_reply": "2026-02-10T06:15:27.624470Z"
    }
   },
   "outputs": [],
   "source": "import numpy as np\nimport pymash as mash"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Data\n",
    "\n",
    "We simulate a larger dataset (40k tests) to mimic a real scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.626404Z",
     "iopub.status.busy": "2026-02-10T06:15:27.626286Z",
     "iopub.status.idle": "2026-02-10T06:15:27.630359Z",
     "shell.execute_reply": "2026-02-10T06:15:27.630013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tests: 40000\n"
     ]
    }
   ],
   "source": [
    "sim = mash.simple_sims(nsamp=10000, ncond=5, err_sd=1.0, seed=1)\n",
    "print(f\"Total tests: {sim['Bhat'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Strong and Random Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.645102Z",
     "iopub.status.busy": "2026-02-10T06:15:27.645001Z",
     "iopub.status.idle": "2026-02-10T06:15:27.656526Z",
     "shell.execute_reply": "2026-02-10T06:15:27.656147Z"
    }
   },
   "outputs": [],
   "source": "# Find strong signals using a quick 1-by-1 analysis\nfull_data = mash.mash_set_data(sim[\"Bhat\"], sim[\"Shat\"])\nm1 = mash.mash_1by1(full_data)\nstrong_idx = mash.get_significant_results(m1, thresh=0.05)\nprint(f\"Strong signals: {len(strong_idx)}\")\n\n# Select a random training subset using the workflow module.\n# For real eQTL data with millions of tests, use n_train=20000-50000.\nrandom_idx = mash.select_training_effects(\n    full_data, n_train=5000, method=\"random\", seed=42,\n)\nprint(f\"Random subset: {len(random_idx)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Estimate Null Correlations\n",
    "\n",
    "Estimate the residual correlation structure among conditions from the random\n",
    "subset. This captures correlations due to confounders, not true effects.\n",
    "\n",
    "**Use the random subset** (not the strong subset, which may lack null tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.657610Z",
     "iopub.status.busy": "2026-02-10T06:15:27.657542Z",
     "iopub.status.idle": "2026-02-10T06:15:27.660015Z",
     "shell.execute_reply": "2026-02-10T06:15:27.659734Z"
    }
   },
   "outputs": [],
   "source": "data_temp = mash.mash_set_data(\n    sim[\"Bhat\"][random_idx], sim[\"Shat\"][random_idx]\n)\nVhat = mash.estimate_null_correlation_simple(data_temp)\nprint(\"Estimated null correlation matrix:\")\nprint(np.array2string(Vhat, precision=3))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Create Data Objects with Estimated Correlations\n\nCreate the full dataset (for fitting and posterior computation) and a\nstrong-signal subset (for covariance learning only)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.661066Z",
     "iopub.status.busy": "2026-02-10T06:15:27.660996Z",
     "iopub.status.idle": "2026-02-10T06:15:27.663938Z",
     "shell.execute_reply": "2026-02-10T06:15:27.663589Z"
    }
   },
   "outputs": [],
   "source": "data_all = mash.mash_set_data(\n    sim[\"Bhat\"], sim[\"Shat\"], V=Vhat\n)\ndata_strong = mash.mash_set_data(\n    sim[\"Bhat\"][strong_idx], sim[\"Shat\"][strong_idx], V=Vhat\n)\nprint(f\"Full data: {data_all.n_effects} tests x {data_all.n_conditions} conditions\")\nprint(f\"Strong data: {data_strong.n_effects} tests x {data_strong.n_conditions} conditions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Learn Data-Driven Covariances from Strong Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:27.665014Z",
     "iopub.status.busy": "2026-02-10T06:15:27.664951Z",
     "iopub.status.idle": "2026-02-10T06:15:36.767155Z",
     "shell.execute_reply": "2026-02-10T06:15:36.766684Z"
    }
   },
   "outputs": [],
   "source": "U_pca = mash.cov_pca(data_strong, npc=5)\nU_ed = mash.cov_ed(data_strong, U_pca)\nprint(f\"Data-driven covariances: {list(U_ed.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Fit the Model on the Random Subset\n\nUse `fit_mash_prior` to fit mixture weights on the random training subset.\nIt handles subsetting internally — pass the full `data_all` and the\n`random_idx` from `select_training_effects`. It fits with `outputlevel=1`\n(mixture weights only, no posteriors)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:15:36.768230Z",
     "iopub.status.busy": "2026-02-10T06:15:36.768153Z",
     "iopub.status.idle": "2026-02-10T06:17:02.756485Z",
     "shell.execute_reply": "2026-02-10T06:17:02.756002Z"
    }
   },
   "outputs": [],
   "source": "U_c = mash.cov_canonical(data_all)\nU_all = {**U_ed, **U_c}\n\nfitted_g, train_idx, train_result = mash.fit_mash_prior(\n    data_all, U_all, train_indices=random_idx,\n)\nprint(f\"Log-likelihood: {train_result.loglik:.2f}\")\nprint(f\"Training subset: {len(train_idx)} tests\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Compute Posteriors on the Strong Subset\n\nUse `apply_mash_prior` to apply the learned model to the strong signals.\nThis calls `mash(..., g=fitted_g, fixg=True)` internally.\n\nYou could also apply to the full dataset or any other subset of interest."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T06:17:02.757828Z",
     "iopub.status.busy": "2026-02-10T06:17:02.757733Z",
     "iopub.status.idle": "2026-02-10T06:17:03.146285Z",
     "shell.execute_reply": "2026-02-10T06:17:03.145803Z"
    }
   },
   "outputs": [],
   "source": "m_strong = mash.apply_mash_prior(data_strong, fitted_g)\n\nsig = mash.get_significant_results(m_strong, thresh=0.05)\nprint(f\"Significant effects in strong set: {len(sig)}\")\nprint(f\"\\nlfsr (first 5 effects):\")\nprint(mash.get_lfsr(m_strong)[:5].round(3))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Loading Real Data from CSV\n\nIn practice, you would load your Bhat and Shat matrices from files.\nFor example, starting from CSV files where rows are tests and columns\nare conditions:\n\n```python\nimport pandas as pd\n\nbhat_df = pd.read_csv(\"bhat.csv\", index_col=0)\nshat_df = pd.read_csv(\"shat.csv\", index_col=0)\n\ndata = mash.mash_set_data(bhat_df.values, shat_df.values)\n```\n\nThe key requirement is that `Bhat` and `Shat` are aligned matrices with\nthe same shape: J tests (rows) by R conditions (columns).\n\n## One-Shot Alternative: `mash_train_apply`\n\nThe entire train/apply workflow (steps 3–4) can be combined into a single\ncall. This is convenient when you don't need separate control over\neach stage:\n\n```python\nresult = mash.mash_train_apply(\n    data_all, U_all,\n    n_train=5000,\n    select_method=\"random\",\n)\npm   = mash.get_pm(result.apply_result)\nlfsr = mash.get_lfsr(result.apply_result)\nprint(f\"Trained on {len(result.train_indices)} tests\")\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}